
SENTENCE-LEVEL FENCING:
    when next buffer token is <eos>, force reduce until |s| = 1, then step sentence-level model to predict next sentence-level repr

while training:
    encoder step
        if shift:
            get buffer item, add to stack
        if reduce:
            pop 2 stack items (zl[A1, X1], zr[A2, X2])
            zl, zr -> zp[Ap=A1:A2:1, Xp=X1:X2] (reduce)
            zl -> ~zp (predict)
            ~zp -> ~A, ~X
            decoder loss: xent(~A, Ap) + xent(~X, Xp)
                backprop back into encoder as well
        encoder loss: abs(S - log_2(n))
        where S is the height of the tree produced by reducing the whole stack and n is the number of shifted items
            not directly differentiable - create output gradient using stepwise difference in loss
            works (hopefully) since stack is temporally consistent
            potentially use encoder loss to guide policy directly? RL techniques
    backprop